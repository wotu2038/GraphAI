from typing import Literal, Optional, AsyncGenerator
from openai import OpenAI
import httpx
import json
from app.core.config import settings
import logging

logger = logging.getLogger(__name__)

LLMProvider = Literal["qianwen", "deepseek", "kimi", "glm"]


class LLMClient:
    def __init__(self):
        self.qianwen_client = None
        self.deepseek_client = None
        self.kimi_client = None
        self.glm_client = None
        self._init_clients()
    
    def _init_clients(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        # åƒé—®ä½¿ç”¨OpenAIå…¼å®¹æ¥å£ï¼ˆæ”¯æŒQWENå’ŒQIANWENä¸¤ç§å‘½åï¼‰
        self.qianwen_api_key = settings.QWEN_API_KEY or settings.QIANWEN_API_KEY
        self.qianwen_api_base = settings.QWEN_API_BASE or settings.QIANWEN_API_BASE
        self.qianwen_model = settings.QWEN_MODEL
        
        # åƒé—®å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨OpenAIå…¼å®¹æ¥å£ï¼‰
        if self.qianwen_api_key:
            try:
                qianwen_base_url = self.qianwen_api_base.rstrip('/')
                if "/compatible-mode/v1" not in qianwen_base_url:
                    if "/compatible-mode" not in qianwen_base_url:
                        qianwen_base_url = f"{qianwen_base_url}/compatible-mode/v1"
                    else:
                        # å¦‚æœå·²ç»æœ‰ compatible-modeï¼Œç¡®ä¿æœ‰ /v1
                        if not qianwen_base_url.endswith("/v1"):
                            qianwen_base_url = f"{qianwen_base_url}/v1"
                self.qianwen_client = OpenAI(
                    api_key=self.qianwen_api_key,
                    base_url=qianwen_base_url
                )
            except Exception as e:
                logger.warning(f"åƒé—®å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.qianwen_client = None
        
        # DeepSeekå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨OpenAIå…¼å®¹æ¥å£ï¼‰
        if settings.DEEPSEEK_API_KEY:
            try:
                deepseek_base_url = settings.DEEPSEEK_API_BASE.rstrip('/')
                if not deepseek_base_url.endswith("/v1"):
                    if "/v1" not in deepseek_base_url:
                        deepseek_base_url = f"{deepseek_base_url}/v1"
                self.deepseek_client = OpenAI(
                    api_key=settings.DEEPSEEK_API_KEY,
                    base_url=deepseek_base_url
                )
            except Exception as e:
                logger.warning(f"DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.deepseek_client = None
        
        # Kimiå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨OpenAIå…¼å®¹æ¥å£ï¼‰
        if settings.KIMI_API_KEY:
            try:
                kimi_base_url = settings.KIMI_API_BASE.rstrip('/')
                if not kimi_base_url.endswith("/v1"):
                    if "/v1" not in kimi_base_url:
                        kimi_base_url = f"{kimi_base_url}/v1"
                self.kimi_client = OpenAI(
                    api_key=settings.KIMI_API_KEY,
                    base_url=kimi_base_url
                )
            except Exception as e:
                logger.warning(f"Kimiå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.kimi_client = None
        
        # GLMå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨OpenAIå…¼å®¹æ¥å£ï¼‰
        if settings.GLM_API_KEY:
            try:
                glm_base_url = settings.GLM_API_BASE.rstrip('/')
                if not glm_base_url.endswith("/v1"):
                    if "/v1" not in glm_base_url:
                        glm_base_url = f"{glm_base_url}/v1"
                self.glm_client = OpenAI(
                    api_key=settings.GLM_API_KEY,
                    base_url=glm_base_url
                )
            except Exception as e:
                logger.warning(f"GLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.glm_client = None
    
    async def chat(
        self,
        provider: LLMProvider,
        messages: list,
        model: Optional[str] = None,
        temperature: float = 0.7,
        use_thinking: bool = False
    ) -> str:
        """è°ƒç”¨LLMè¿›è¡Œå¯¹è¯"""
        if provider == "qianwen":
            return await self._chat_qianwen(messages, model, temperature)
        elif provider == "deepseek":
            return await self._chat_deepseek(messages, model, temperature)
        elif provider == "kimi":
            return await self._chat_kimi(messages, model, temperature)
        elif provider == "glm":
            return await self._chat_glm(messages, model, temperature)
        else:
            raise ValueError(f"Unsupported provider: {provider}")
    
    async def _chat_qianwen(
        self,
        messages: list,
        model: Optional[str],
        temperature: float
    ) -> str:
        """åƒé—®å¯¹è¯ï¼ˆOpenAIå…¼å®¹æ¥å£ï¼‰"""
        if not self.qianwen_client:
            raise ValueError("åƒé—® API æœªé…ç½®")
        
        model = model or self.qianwen_model
        if not model:
            raise ValueError("åƒé—® model æœªé…ç½®")
        
        # è·å– base_url
        qianwen_base_url = self.qianwen_api_base.rstrip('/')
        if "/compatible-mode/v1" not in qianwen_base_url:
            if "/compatible-mode" not in qianwen_base_url:
                qianwen_base_url = f"{qianwen_base_url}/compatible-mode/v1"
            else:
                if not qianwen_base_url.endswith("/v1"):
                    qianwen_base_url = f"{qianwen_base_url}/v1"
        
        logger.info(f"ğŸŸ¢ è°ƒç”¨åƒé—® API: model={model}, base_url={qianwen_base_url}, temperature={temperature}")
        
        # ä½¿ç”¨å¼‚æ­¥æ–¹å¼è°ƒç”¨
        import asyncio
        loop = asyncio.get_event_loop()
        try:
            response = await loop.run_in_executor(
                None,
                lambda: self.qianwen_client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature
                )
            )
            logger.info(f"âœ… åƒé—® API è°ƒç”¨æˆåŠŸ: model={model}")
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"âŒ åƒé—® API è°ƒç”¨å¤±è´¥: {e}")
            raise
    
    async def _chat_deepseek(
        self,
        messages: list,
        model: Optional[str],
        temperature: float
    ) -> str:
        """DeepSeekå¯¹è¯ï¼ˆOpenAIå…¼å®¹æ¥å£ï¼‰"""
        if not self.deepseek_client:
            raise ValueError("DeepSeek API æœªé…ç½®")
        
        model = model or settings.DEEPSEEK_MODEL
        if not model:
            raise ValueError("DeepSeek model æœªé…ç½®")
        
        logger.info(f"ğŸ”µ è°ƒç”¨ DeepSeek API: model={model}, base_url={settings.DEEPSEEK_API_BASE}, temperature={temperature}")
        
        # ä½¿ç”¨å¼‚æ­¥æ–¹å¼è°ƒç”¨
        import asyncio
        loop = asyncio.get_event_loop()
        try:
            response = await loop.run_in_executor(
                None,
                lambda: self.deepseek_client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature
                )
            )
            logger.info(f"âœ… DeepSeek API è°ƒç”¨æˆåŠŸ: model={model}")
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"âŒ DeepSeek API è°ƒç”¨å¤±è´¥: {e}")
            raise
    
    async def _chat_kimi(
        self,
        messages: list,
        model: Optional[str],
        temperature: float
    ) -> str:
        """Kimiå¯¹è¯ï¼ˆOpenAIå…¼å®¹æ¥å£ï¼‰"""
        if not self.kimi_client:
            raise ValueError("Kimi API æœªé…ç½®")
        
        model = model or settings.KIMI_MODEL
        if not model:
            raise ValueError("Kimi model æœªé…ç½®")
        
        logger.info(f"ğŸŸ£ è°ƒç”¨ Kimi API: model={model}, base_url={settings.KIMI_API_BASE}, temperature={temperature}")
        
        # ä½¿ç”¨å¼‚æ­¥æ–¹å¼è°ƒç”¨
        import asyncio
        loop = asyncio.get_event_loop()
        try:
            response = await loop.run_in_executor(
                None,
                lambda: self.kimi_client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature
                )
            )
            logger.info(f"âœ… Kimi API è°ƒç”¨æˆåŠŸ: model={model}")
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"âŒ Kimi API è°ƒç”¨å¤±è´¥: {e}")
            raise
    
    async def _chat_glm(
        self,
        messages: list,
        model: Optional[str],
        temperature: float
    ) -> str:
        """GLMå¯¹è¯ï¼ˆOpenAIå…¼å®¹æ¥å£ï¼‰"""
        if not self.glm_client:
            raise ValueError("GLM API æœªé…ç½®")
        
        model = model or settings.GLM_MODEL
        if not model:
            raise ValueError("GLM model æœªé…ç½®")
        
        logger.info(f"ğŸŸ¢ è°ƒç”¨ GLM API: model={model}, base_url={settings.GLM_API_BASE}, temperature={temperature}")
        
        # ä½¿ç”¨å¼‚æ­¥æ–¹å¼è°ƒç”¨
        import asyncio
        loop = asyncio.get_event_loop()
        try:
            response = await loop.run_in_executor(
                None,
                lambda: self.glm_client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature
                )
            )
            logger.info(f"âœ… GLM API è°ƒç”¨æˆåŠŸ: model={model}")
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"âŒ GLM API è°ƒç”¨å¤±è´¥: {e}")
            raise
    
    async def _chat_local(
        self,
        messages: list,
        model: Optional[str],
        temperature: float,
        use_thinking: bool = False
    ) -> str:
        """æœ¬åœ°å¤§æ¨¡å‹å¯¹è¯ï¼ˆOpenAIå…¼å®¹æ¥å£ï¼‰"""
        if not self.local_client:
            raise ValueError("æœ¬åœ°å¤§æ¨¡å‹ API æœªé…ç½®")
        
        model = model or settings.LOCAL_LLM_MODEL
        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = {
            "model": model,
            "messages": messages,
            "temperature": temperature
        }
        
        # å¦‚æœå¯ç”¨Thinkingæ¨¡å¼ï¼Œæ·»åŠ extra_bodyå‚æ•°
        if use_thinking:
            request_params["extra_body"] = {"thinking": True}
            logger.info(f"å¯ç”¨Thinkingæ¨¡å¼: extra_body={request_params['extra_body']}")
        else:
            logger.info(f"æœªå¯ç”¨Thinkingæ¨¡å¼ (use_thinking={use_thinking})")
        
        # ä½¿ç”¨å¼‚æ­¥æ–¹å¼è°ƒç”¨
        import asyncio
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: self.local_client.chat.completions.create(**request_params)
        )
        return response.choices[0].message.content
    
    async def chat_stream(
        self,
        provider: LLMProvider,
        messages: list,
        model: Optional[str] = None,
        temperature: float = 0.7,
        use_thinking: bool = False
    ) -> AsyncGenerator[str, None]:
        """æµå¼è°ƒç”¨LLMè¿›è¡Œå¯¹è¯ï¼ˆæ”¯æŒ OpenAI å…¼å®¹æ¥å£çš„æ¨¡å‹ï¼‰"""
        if provider == "qianwen":
            async for chunk in self._chat_qianwen_stream(messages, model, temperature):
                yield chunk
        elif provider == "deepseek":
            async for chunk in self._chat_deepseek_stream(messages, model, temperature):
                yield chunk
        elif provider == "kimi":
            async for chunk in self._chat_kimi_stream(messages, model, temperature):
                yield chunk
        else:
            raise ValueError(f"æµå¼è¾“å‡ºä¸æ”¯æŒ provider: {provider}ï¼Œæ”¯æŒçš„ provider: qianwen, deepseek, kimi")
    
    async def _chat_qianwen_stream(
        self,
        messages: list,
        model: Optional[str],
        temperature: float
    ) -> AsyncGenerator[str, None]:
        """åƒé—®æµå¼å¯¹è¯ï¼ˆOpenAIå…¼å®¹æ¥å£ï¼‰"""
        if not self.qianwen_client:
            raise ValueError("åƒé—® API æœªé…ç½®")
        
        model = model or self.qianwen_model
        if not model:
            raise ValueError("åƒé—® model æœªé…ç½®")
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "stream": True  # å¯ç”¨æµå¼è¾“å‡º
        }
        
        logger.info(f"ğŸŸ¢ è°ƒç”¨åƒé—®æµå¼ API: model={model}, temperature={temperature}")
        
        # ä½¿ç”¨å¼‚æ­¥æ–¹å¼è°ƒç”¨æµå¼æ¥å£
        import asyncio
        loop = asyncio.get_event_loop()
        
        # åˆ›å»ºæµå¼å“åº”ï¼ˆåŒæ­¥è°ƒç”¨ï¼Œä½†è¿”å›çš„æ˜¯æµå¯¹è±¡ï¼‰
        def create_stream():
            return self.qianwen_client.chat.completions.create(**request_params)
        
        stream = await loop.run_in_executor(None, create_stream)
        
        # å¼‚æ­¥è¿­ä»£æµå¼å“åº”
        # æ³¨æ„ï¼šOpenAIçš„æµå¼å“åº”æ˜¯åŒæ­¥è¿­ä»£å™¨ï¼Œéœ€è¦åœ¨executorä¸­è¿­ä»£
        import queue
        q = queue.Queue()
        exception_holder = [None]
        finished = False
        
        def sync_iterate():
            nonlocal finished
            try:
                for chunk in stream:
                    if chunk.choices and len(chunk.choices) > 0:
                        delta = chunk.choices[0].delta
                        if delta and delta.content:
                            q.put(delta.content)
                finished = True
                q.put(None)  # ç»“æŸæ ‡è®°
            except Exception as e:
                exception_holder[0] = e
                finished = True
                q.put(None)
        
        # åœ¨åå°çº¿ç¨‹ä¸­è¿­ä»£
        loop.run_in_executor(None, sync_iterate)
        
        # å¼‚æ­¥ä»é˜Ÿåˆ—ä¸­è·å–å†…å®¹
        while not finished or not q.empty():
            try:
                # ä½¿ç”¨è¶…æ—¶é¿å…æ— é™ç­‰å¾…
                content = await asyncio.get_event_loop().run_in_executor(
                    None, lambda: q.get(timeout=0.5)
                )
                if content is None:
                    if exception_holder[0]:
                        raise exception_holder[0]
                    break
                yield content
            except queue.Empty:
                # è¶…æ—¶ï¼Œç»§ç»­ç­‰å¾…ï¼ˆä½†æ£€æŸ¥æ˜¯å¦å·²å®Œæˆï¼‰
                if finished and q.empty():
                    break
                await asyncio.sleep(0.05)
                continue
    
    async def _chat_deepseek_stream(
        self,
        messages: list,
        model: Optional[str],
        temperature: float
    ) -> AsyncGenerator[str, None]:
        """DeepSeekæµå¼å¯¹è¯ï¼ˆOpenAIå…¼å®¹æ¥å£ï¼‰"""
        if not self.deepseek_client:
            raise ValueError("DeepSeek API æœªé…ç½®")
        
        model = model or settings.DEEPSEEK_MODEL
        if not model:
            raise ValueError("DeepSeek model æœªé…ç½®")
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "stream": True  # å¯ç”¨æµå¼è¾“å‡º
        }
        
        logger.info(f"ğŸ”µ è°ƒç”¨ DeepSeek æµå¼ API: model={model}, temperature={temperature}")
        
        # ä½¿ç”¨å¼‚æ­¥æ–¹å¼è°ƒç”¨æµå¼æ¥å£
        import asyncio
        loop = asyncio.get_event_loop()
        
        # åˆ›å»ºæµå¼å“åº”ï¼ˆåŒæ­¥è°ƒç”¨ï¼Œä½†è¿”å›çš„æ˜¯æµå¯¹è±¡ï¼‰
        def create_stream():
            return self.deepseek_client.chat.completions.create(**request_params)
        
        stream = await loop.run_in_executor(None, create_stream)
        
        # å¼‚æ­¥è¿­ä»£æµå¼å“åº”
        import queue
        q = queue.Queue()
        exception_holder = [None]
        finished = False
        
        def sync_iterate():
            nonlocal finished
            try:
                for chunk in stream:
                    if chunk.choices and len(chunk.choices) > 0:
                        delta = chunk.choices[0].delta
                        if delta and delta.content:
                            q.put(delta.content)
                finished = True
                q.put(None)  # ç»“æŸæ ‡è®°
            except Exception as e:
                exception_holder[0] = e
                finished = True
                q.put(None)
        
        # åœ¨åå°çº¿ç¨‹ä¸­è¿­ä»£
        loop.run_in_executor(None, sync_iterate)
        
        # å¼‚æ­¥ä»é˜Ÿåˆ—ä¸­è·å–å†…å®¹
        while not finished or not q.empty():
            try:
                content = await asyncio.get_event_loop().run_in_executor(
                    None, lambda: q.get(timeout=0.5)
                )
                if content is None:
                    if exception_holder[0]:
                        raise exception_holder[0]
                    break
                yield content
            except queue.Empty:
                if finished and q.empty():
                    break
                await asyncio.sleep(0.05)
                continue
    
    async def _chat_kimi_stream(
        self,
        messages: list,
        model: Optional[str],
        temperature: float
    ) -> AsyncGenerator[str, None]:
        """Kimiæµå¼å¯¹è¯ï¼ˆOpenAIå…¼å®¹æ¥å£ï¼‰"""
        if not self.kimi_client:
            raise ValueError("Kimi API æœªé…ç½®")
        
        model = model or settings.KIMI_MODEL
        if not model:
            raise ValueError("Kimi model æœªé…ç½®")
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "stream": True  # å¯ç”¨æµå¼è¾“å‡º
        }
        
        logger.info(f"ğŸŸ£ è°ƒç”¨ Kimi æµå¼ API: model={model}, temperature={temperature}")
        
        # ä½¿ç”¨å¼‚æ­¥æ–¹å¼è°ƒç”¨æµå¼æ¥å£
        import asyncio
        loop = asyncio.get_event_loop()
        
        # åˆ›å»ºæµå¼å“åº”ï¼ˆåŒæ­¥è°ƒç”¨ï¼Œä½†è¿”å›çš„æ˜¯æµå¯¹è±¡ï¼‰
        def create_stream():
            return self.kimi_client.chat.completions.create(**request_params)
        
        stream = await loop.run_in_executor(None, create_stream)
        
        # å¼‚æ­¥è¿­ä»£æµå¼å“åº”
        import queue
        q = queue.Queue()
        exception_holder = [None]
        finished = False
        
        def sync_iterate():
            nonlocal finished
            try:
                for chunk in stream:
                    if chunk.choices and len(chunk.choices) > 0:
                        delta = chunk.choices[0].delta
                        if delta and delta.content:
                            q.put(delta.content)
                finished = True
                q.put(None)  # ç»“æŸæ ‡è®°
            except Exception as e:
                exception_holder[0] = e
                finished = True
                q.put(None)
        
        # åœ¨åå°çº¿ç¨‹ä¸­è¿­ä»£
        loop.run_in_executor(None, sync_iterate)
        
        # å¼‚æ­¥ä»é˜Ÿåˆ—ä¸­è·å–å†…å®¹
        while not finished or not q.empty():
            try:
                content = await asyncio.get_event_loop().run_in_executor(
                    None, lambda: q.get(timeout=0.5)
                )
                if content is None:
                    if exception_holder[0]:
                        raise exception_holder[0]
                    break
                yield content
            except queue.Empty:
                if finished and q.empty():
                    break
                await asyncio.sleep(0.05)
                continue
    
    async def extract_entities(
        self,
        provider: LLMProvider,
        text: str
    ) -> dict:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»"""
        prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œè¿”å›JSONæ ¼å¼ï¼š
{{
  "entities": [
    {{"name": "å®ä½“åç§°", "type": "å®ä½“ç±»å‹", "properties": {{"key": "value"}}}}
  ],
  "relationships": [
    {{"source": "æºå®ä½“", "target": "ç›®æ ‡å®ä½“", "type": "å…³ç³»ç±»å‹", "properties": {{}}}}
  ]
}}

æ–‡æœ¬å†…å®¹ï¼š
{text}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±å®ä½“æŠ½å–ä¸“å®¶ï¼Œæ“…é•¿ä»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        response = await self.chat(provider, messages, temperature=0.3, use_thinking=False)
        
        # è§£æJSONå“åº”
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            return json.loads(response)
        except json.JSONDecodeError:
            logger.error(f"Failed to parse LLM response: {response}")
            return {"entities": [], "relationships": []}
    
    async def generate(
        self,
        provider: LLMProvider,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        use_thinking: bool = False
    ) -> str:
        """ç”Ÿæˆæ–‡æœ¬ï¼ˆåŸºäºpromptï¼‰"""
        messages = [
            {"role": "user", "content": prompt}
        ]
        return await self.chat(provider, messages, temperature=temperature, use_thinking=use_thinking)


llm_client = LLMClient()


def get_llm_client(provider: Optional[LLMProvider] = None) -> LLMClient:
    """è·å–LLMå®¢æˆ·ç«¯å®ä¾‹"""
    return llm_client

